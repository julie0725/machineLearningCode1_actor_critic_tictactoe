{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86a3f29",
   "metadata": {},
   "source": [
    "1. 라이브러리 및 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11f34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88744958",
   "metadata": {},
   "source": [
    "2. Tic Tac Toe 환경 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41056df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "   \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 9개의 배열로 준비함\n",
    "    # 게임종료 : done = True\n",
    "#         self.board_a = np.zeros(9)\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "   \n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "            \n",
    "       \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "       \n",
    "        return  self.reward, self.done\n",
    " \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        \n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "   \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self,player):\n",
    "        # 0   1   2   3\n",
    "        # 4   5   6   7\n",
    "        # 8   9  10  11\n",
    "        #12  13  14  15\n",
    "        \n",
    "        # 4 x 4\n",
    "        end_condition = [  \n",
    "            # 가로\n",
    "            (0,1,2,3), (4,5,6,7), (8,9,10,11), (12,13,14,15),\n",
    "            # 세로\n",
    "            (0,4,8,12), (1,5,9,13), (2,6,10,14), (3,7,11,15),\n",
    "            # 대각선\n",
    "            (0,5,10,15), (3,6,9,12)\n",
    "        ]\n",
    "        for line in end_condition:\n",
    "             if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                and self.board_a[line[2]] == self.board_a[line[3]] \\\n",
    "                and self.board_a[line[0]] != 0:  \n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "       \n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self): #4x4로 수정 완료\n",
    "        print(\"+----+----+----+----+\")  \n",
    "        for i in range(4): # 줄 수 \n",
    "            for j in range(4):  # 칸 수 \n",
    "                if self.board_a[4*i+j] == 1:  #index 계산식도 4*i+j로 바꿈\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[4*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")  # 출력 테두리 4칸용으로 바꿈 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b87d4",
   "metadata": {},
   "source": [
    "3. Human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4dfd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 4x4 상태 번호 표시로 수정 완료\n",
    "            print(\"+----+----+----+----+\")  \n",
    "            print(\"+  0 +  1 +  2 +  3 +\") \n",
    "            print(\"+----+----+----+----+\") \n",
    "            print(\"+  4 +  5 +  6 +  7 +\")  \n",
    "            print(\"+----+----+----+----+\")  \n",
    "            print(\"+  8 +  9 + 10 + 11 +\")  \n",
    "            print(\"+----+----+----+----+\") \n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")  \n",
    "            print(\"+----+----+----+----+\")  \n",
    "    \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcf631",
   "metadata": {},
   "source": [
    "4. Actor-Critic 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934d8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticPlayer:\n",
    "    def __init__(self):\n",
    "        self.name = \"ActorCritic\"\n",
    "        self.V = {}        # 크리틱: 상태 가치 함수 V(s)\n",
    "        self.policy = {}   # 액터: 상태-행동 확률 정책 ㅠ(s,a)\n",
    "        self.alpha = 0.1   # 크리틱: 상태 가치 업데이트 속도   ✅ 이상하다싶으면 원상복귀\n",
    "        self.beta = 0.4    # <- 이전) 0.2   주의) 얘는 유지 바꾸니 변동이 커짐\n",
    "                           # 액터: 정책 학습률 0.01 -> 0.1->0.2 ->0.4로 수정함\n",
    "                           #       정책 업데이트 속도 (actor)\n",
    "                           # TD 오차에 따라 정책 빠르게 업데이트 가능하도록...\n",
    "        self.gamma = 0.9   # 감가율 : 미래 보상 중요도  (0.93으로 강화해 디버깅해봤을 때 td 에러 더 큼. 따라서 0.9로 유지 )\n",
    "      \n",
    "    def get_state_key(self, board):\n",
    "         # 상태를 딕셔너리 key로 사용하기 위해 튜플로 변환\n",
    "        return tuple(board.astype(int))\n",
    "\n",
    "    def softmax(self, probs):\n",
    "        # Softmax: 확률 분포 계산 (확률값 정규화)\n",
    "        exp_probs = np.exp(probs - np.max(probs))  # overflow 방지\n",
    "        return exp_probs / np.sum(exp_probs)\n",
    "\n",
    "    def select_action(self, env, player):\n",
    "        # 현재 상태에서 행동 선택\n",
    "        state_key = self.get_state_key(env.board_a)\n",
    "        available_actions = env.get_action()\n",
    "\n",
    "        # 정책이 없다면: 무작위로 초기화하기 (사용 가능한 행동만)\n",
    "        if state_key not in self.policy:\n",
    "            self.policy[state_key] = np.zeros(16)\n",
    "            for i in available_actions:\n",
    "                self.policy[state_key][i] = np.random.rand()  # 균등 초기화보다 빠르게 수렴 유도\n",
    "\n",
    "        # 소프트맥스 이전 값 \n",
    "        logits = self.policy[state_key].copy() #probs 변수명 -> logits로 바꿈 : 소프트맥스 적용 전 값이므로, 좀더 이해하기 편하게 하기 위한 용도 \n",
    "\n",
    "        \n",
    "        # 불가능한 행동은 -np.inf로 (액크플레이어가 덮어쓰는 걸 방지하기 위함)\n",
    "        for i in range(16):\n",
    "            if i not in available_actions:\n",
    "                logits[i] = -np.inf\n",
    "                \n",
    "        # Gibbs softmax로 확률로 변환하기 (정규화)\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        # 확률에 따라 행동 선택(Gibbs 정책)\n",
    "        action = np.random.choice(range(16), p=probs)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # 학습 단계) Actor-Critic 동시 업데이트하기\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        # 상태 가치 함수 및 정책 초기화\n",
    "        if state_key not in self.V:\n",
    "            self.V[state_key] = 0.0\n",
    "        if next_state_key not in self.V:\n",
    "            self.V[next_state_key] = 0.0\n",
    "        if state_key not in self.policy:\n",
    "            self.policy[state_key] = np.zeros(16)\n",
    "            for i in range(16):\n",
    "                self.policy[state_key][i] = np.random.rand()\n",
    "                \n",
    "        # [Critic] 학습 ----------------------------------------------\n",
    "        # TD error는 δt=r(t+1)+γV(S(t+1) )-V(St)\n",
    "        # 1) TD target 계산\n",
    "        if done:\n",
    "            td_target = reward # 게임 종료 시: 다음 상태 고려 X\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.V[next_state_key] # td_target\n",
    "        # 2) TD error\n",
    "        td_error = td_target - self.V[state_key] # TD 오차 = 실제 - 예측\n",
    "\n",
    "        # Critic 가치 함수 업데이트 (TD 학습)\n",
    "        self.V[state_key] += self.alpha * td_error\n",
    "\n",
    "        # [Actor] 학습 : 정책 업데이트 (TD 오차에 따라 정책 수정) ------\n",
    "        \n",
    "        self.policy[state_key][action] += self.beta * td_error\n",
    "\n",
    "        # 확률 음수 방지 + Softmax 다시 정규화\n",
    "        self.policy[state_key] = np.maximum(self.policy[state_key], 1e-5)\n",
    "        self.policy[state_key] = self.softmax(self.policy[state_key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f4e5d",
   "metadata": {},
   "source": [
    "5. Actor-Critic 플레이어 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0152275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Actor-Critic Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████▏                                      | 95094/200000 [07:17<08:08, 214.81it/s]"
     ]
    }
   ],
   "source": [
    "# 학습 에이전트 두 명 생성\n",
    "p1 = ActorCriticPlayer()\n",
    "p2 = ActorCriticPlayer()\n",
    "\n",
    "# 학습 에피소드 수 \n",
    "# episodes = 30000  # 학습 반복 횟수\n",
    "# episodes = 70000\n",
    "episodes  = 200000 # 100000 잠시 대기 \n",
    "\n",
    "print(\"Start Actor-Critic Training...\")\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    env = Environment()\n",
    "    env.print = False  # 학습 시에는 출력 생략\n",
    "\n",
    "    # 초기 상태 저장\n",
    "    state = env.board_a.copy()\n",
    "\n",
    "    for turn in range(100):\n",
    "        # 현재 플레이어 결정 (1 또는 -1)\n",
    "        player = 1 if turn % 2 == 0 else -1\n",
    "        current_player = p1 if player == 1 else p2\n",
    "\n",
    "        # 행동 선택\n",
    "        action = current_player.select_action(env, player)\n",
    "\n",
    "        # 행동 적용\n",
    "        env.board_a[action] = player\n",
    "\n",
    "        # 종료 여부 확인\n",
    "        env.end_check(player)\n",
    "        \n",
    "        reward = env.reward\n",
    "        done = env.done\n",
    "        next_state = env.board_a.copy()\n",
    "\n",
    "        if done:\n",
    "            if reward == player:\n",
    "                r = 10       # 승리 보상 강화  (반복 끝에 승리보상을 강화하기)\n",
    "            elif reward == 0:\n",
    "                r = 1        # 무승부 보상 완화 (10배 강화하니까 비교적 학습 O)\n",
    "            else:\n",
    "                r = -10      # 패배 패널티 강화 \n",
    "        else:\n",
    "            r = -0.5     # 진행 중 패널티 강화 (-0.05값을 -0.1~1.0으로 조정 실험후 -0.5로 고정)\n",
    "            \n",
    "\n",
    "        # 업데이트 수행\n",
    "        current_player.update(state, action, r, next_state, done)\n",
    "\n",
    "        # 다음 상태로 전이 \n",
    "        state = next_state.copy()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "print(\"Training finished.\")\n",
    "\n",
    "trained_p2 = p2\n",
    "\n",
    "\n",
    "print(\"총 학습된 상태 수:\", len(p2.V))\n",
    "print(\"예시 상태 가치:\", list(p2.V.items())[:3])\n",
    "print(\"예시 상태 정책:\", list(p2.policy.items())[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e62b1",
   "metadata": {},
   "source": [
    "  6.게임 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59481380",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "p1 = Human_player()\n",
    "# p1 = trained_p1\n",
    "p2 = trained_p2\n",
    "# p2 = Human_player()\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
